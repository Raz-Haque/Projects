# -*- coding: utf-8 -*-
"""Disney Remake NLP Exploration

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vn-n8b5WaAavy942_TRsVgIkH-25HJI1
"""

!pip install praw
import praw
import pprint
secret="K-023J8b3hUeew_vvP1tg9fZPWoMfg"
APP_ID="Cjjl_-MnS2K8_YcfXQVpZg"
reddit= praw.Reddit(
    check_for_async=False,
    client_id=APP_ID,
    client_secret=secret,
    user_agent="Comment Extraction")
submission_url_SnowWhite_1 = "https://www.reddit.com/r/disney/comments/1jg0m33/official_rdisney_snow_white_2025_discussion/"
submission_SnowWhite_1 = reddit.submission(url=submission_url_SnowWhite_1)
# Ensure comments are fully loaded
submission_SnowWhite_1.comments.replace_more(limit=None)

# Flatten all comments in the comment forest
all_comments_SnowWhite_1 = submission_SnowWhite_1.comments.list()

# Initialize an empty list to store comment data
comment_data_SnowWhite_1 = []

# Extract and print details for 100 comments
for i, comment in enumerate(all_comments_SnowWhite_1[:100]):  # Get first 100 comments
    author = comment.author.name if comment.author else "[Deleted]"  # Handle deleted authors
    comment_body = comment.body
    word_count = len(comment_body.split())
    comment_data_SnowWhite_1.append({
        "Author": author,
        "Comment": comment_body,
        "Word Count": word_count,
        "Score": comment.score
    })
# Step 6: Convert the list into a Pandas DataFrame
import pandas as pd  # Import pandas for DataFrame
Snow_White_1= pd.DataFrame(comment_data_SnowWhite_1)

secret="K-023J8b3hUeew_vvP1tg9fZPWoMfg"
APP_ID="Cjjl_-MnS2K8_YcfXQVpZg"
reddit= praw.Reddit(
    check_for_async=False,
    client_id=APP_ID,
    client_secret=secret,
    user_agent="Comment Extraction")
submission_url_SnowWhite_2 = "https://www.reddit.com/r/disney/comments/1ji1wwd/box_office_snow_white_limps_to_worrisome_43m/"
submission_SnowWhite_2 = reddit.submission(url=submission_url_SnowWhite_2)
# Ensure comments are fully loaded
submission_SnowWhite_2.comments.replace_more(limit=None)

# Flatten all comments in the comment forest
all_comments_SnowWhite_2 = submission_SnowWhite_2.comments.list()

# Initialize an empty list to store comment data
comment_data_SnowWhite_2 = []

# Extract and print details for 100 comments
for i, comment in enumerate(all_comments_SnowWhite_2[:100]):  # Get first 100 comments
    author = comment.author.name if comment.author else "[Deleted]"  # Handle deleted authors
    comment_body = comment.body
    word_count = len(comment_body.split())
    comment_data_SnowWhite_2.append({
        "Author": author,
        "Comment": comment_body,
        "Word Count": word_count,
        "Score": comment.score
    })
# Step 6: Convert the list into a Pandas DataFrame
import pandas as pd  # Import pandas for DataFrame
Snow_White_2= pd.DataFrame(comment_data_SnowWhite_2)

secret="K-023J8b3hUeew_vvP1tg9fZPWoMfg"
APP_ID="Cjjl_-MnS2K8_YcfXQVpZg"
reddit= praw.Reddit(
    check_for_async=False,
    client_id=APP_ID,
    client_secret=secret,
    user_agent="Comment Extraction")
submission_url_SnowWhite_3 = "https://www.reddit.com/r/disney/comments/1h5pugd/new_official_poster_for_snow_white/"
submission_SnowWhite_3 = reddit.submission(url=submission_url_SnowWhite_3)
# Ensure comments are fully loaded
submission_SnowWhite_3.comments.replace_more(limit=None)

# Flatten all comments in the comment forest
all_comments_SnowWhite_3 = submission_SnowWhite_3.comments.list()

# Initialize an empty list to store comment data
comment_data_SnowWhite_3 = []

# Extract and print details for 100 comments
for i, comment in enumerate(all_comments_SnowWhite_3[:100]):  # Get first 100 comments
    author = comment.author.name if comment.author else "[Deleted]"  # Handle deleted authors
    comment_body = comment.body
    word_count = len(comment_body.split())
    comment_data_SnowWhite_3.append({
        "Author": author,
        "Comment": comment_body,
        "Word Count": word_count,
        "Score": comment.score
    })
# Step 6: Convert the list into a Pandas DataFrame
import pandas as pd  # Import pandas for DataFrame
Snow_White_3= pd.DataFrame(comment_data_SnowWhite_3)

secret="K-023J8b3hUeew_vvP1tg9fZPWoMfg"
APP_ID="Cjjl_-MnS2K8_YcfXQVpZg"
reddit= praw.Reddit(
    check_for_async=False,
    client_id=APP_ID,
    client_secret=secret,
    user_agent="Comment Extraction")
submission_url_LiloAndStitch_1 = "https://www.reddit.com/r/disney/comments/1kt79ny/official_rdisney_lilo_stitch_2025_discussion/"
submission_LiloAndStitch_1 = reddit.submission(url=submission_url_LiloAndStitch_1)
# Ensure comments are fully loaded
submission_LiloAndStitch_1.comments.replace_more(limit=None)

# Flatten all comments in the comment forest
all_comments_LiloAndStitch_1 = submission_LiloAndStitch_1.comments.list()

# Initialize an empty list to store comment data
comment_data_LiloAndStitch_1 = []

# Extract and print details for 100 comments
for i, comment in enumerate(all_comments_LiloAndStitch_1[:100]):  # Get first 100 comments
    author = comment.author.name if comment.author else "[Deleted]"  # Handle deleted authors
    comment_body = comment.body
    word_count = len(comment_body.split())
    comment_data_LiloAndStitch_1.append({
        "Author": author,
        "Comment": comment_body,
        "Word Count": word_count,
        "Score": comment.score
    })
# Step 6: Convert the list into a Pandas DataFrame
import pandas as pd  # Import pandas for DataFrame
LiloAndStitch_1= pd.DataFrame(comment_data_LiloAndStitch_1)

secret="K-023J8b3hUeew_vvP1tg9fZPWoMfg"
APP_ID="Cjjl_-MnS2K8_YcfXQVpZg"
reddit= praw.Reddit(
    check_for_async=False,
    client_id=APP_ID,
    client_secret=secret,
    user_agent="Comment Extraction")
submission_url_LiloAndStitch_2 = "https://www.reddit.com/r/disney/comments/1kuhbnz/lilo_stitch_director_explains_why_captain_gantu/"
submission_LiloAndStitch_2 = reddit.submission(url=submission_url_LiloAndStitch_2)
# Ensure comments are fully loaded
submission_LiloAndStitch_2.comments.replace_more(limit=None)

# Flatten all comments in the comment forest
all_comments_LiloAndStitch_2 = submission_LiloAndStitch_2.comments.list()

# Initialize an empty list to store comment data
comment_data_LiloAndStitch_2 = []

# Extract and print details for 100 comments
for i, comment in enumerate(all_comments_LiloAndStitch_2[:100]):  # Get first 100 comments
    author = comment.author.name if comment.author else "[Deleted]"  # Handle deleted authors
    comment_body = comment.body
    word_count = len(comment_body.split())
    comment_data_LiloAndStitch_2.append({
        "Author": author,
        "Comment": comment_body,
        "Word Count": word_count,
        "Score": comment.score
    })
# Step 6: Convert the list into a Pandas DataFrame
import pandas as pd  # Import pandas for DataFrame
LiloAndStitch_2= pd.DataFrame(comment_data_LiloAndStitch_2)

secret="K-023J8b3hUeew_vvP1tg9fZPWoMfg"
APP_ID="Cjjl_-MnS2K8_YcfXQVpZg"
reddit= praw.Reddit(
    check_for_async=False,
    client_id=APP_ID,
    client_secret=secret,
    user_agent="Comment Extraction")
submission_url_LiloAndStitch_3 = "https://www.reddit.com/r/disney/comments/1gxe8o7/official_poster_for_the_liveaction_lilo_stitch/"
submission_LiloAndStitch_3 = reddit.submission(url=submission_url_LiloAndStitch_3)
# Ensure comments are fully loaded
submission_LiloAndStitch_3.comments.replace_more(limit=None)

# Flatten all comments in the comment forest
all_comments_LiloAndStitch_3 = submission_LiloAndStitch_3.comments.list()

# Initialize an empty list to store comment data
comment_data_LiloAndStitch_3 = []

# Extract and print details for 100 comments
for i, comment in enumerate(all_comments_LiloAndStitch_3[:100]):  # Get first 100 comments
    author = comment.author.name if comment.author else "[Deleted]"  # Handle deleted authors
    comment_body = comment.body
    word_count = len(comment_body.split())
    comment_data_LiloAndStitch_3.append({
        "Author": author,
        "Comment": comment_body,
        "Word Count": word_count,
        "Score": comment.score
    })
# Step 6: Convert the list into a Pandas DataFrame
import pandas as pd  # Import pandas for DataFrame
LiloAndStitch_3= pd.DataFrame(comment_data_LiloAndStitch_3)

secret="K-023J8b3hUeew_vvP1tg9fZPWoMfg"
APP_ID="Cjjl_-MnS2K8_YcfXQVpZg"
reddit= praw.Reddit(
    check_for_async=False,
    client_id=APP_ID,
    client_secret=secret,
    user_agent="Comment Extraction")
submission_url_LittleMermaid = "https://www.reddit.com/r/disney/comments/13qt9sy/official_rdisney_the_little_mermaid_2023/"
submission_LittleMermaid = reddit.submission(url=submission_url_LittleMermaid)
# Ensure comments are fully loaded
submission_LittleMermaid.comments.replace_more(limit=None)

# Flatten all comments in the comment forest
all_comments_LittleMermaid = submission_LittleMermaid.comments.list()

# Initialize an empty list to store comment data
comment_data_LittleMermaid = []

# Extract and print details for 100 comments
for i, comment in enumerate(all_comments_LittleMermaid[:100]):  # Get first 100 comments
    author = comment.author.name if comment.author else "[Deleted]"  # Handle deleted authors
    comment_body = comment.body
    word_count = len(comment_body.split())
    comment_data_LittleMermaid.append({
        "Author": author,
        "Comment": comment_body,
        "Word Count": word_count,
        "Score": comment.score
    })
# Step 6: Convert the list into a Pandas DataFrame
import pandas as pd  # Import pandas for DataFrame
LittleMermaid= pd.DataFrame(comment_data_LittleMermaid)

secret="K-023J8b3hUeew_vvP1tg9fZPWoMfg"
APP_ID="Cjjl_-MnS2K8_YcfXQVpZg"
reddit= praw.Reddit(
    check_for_async=False,
    client_id=APP_ID,
    client_secret=secret,
    user_agent="Comment Extraction")
submission_url_LittleMermaid_2 = "https://www.reddit.com/r/disney/comments/xaa6xd/the_little_mermaid_official_teaser_trailer/"
submission_LittleMermaid_2 = reddit.submission(url=submission_url_LittleMermaid_2)
# Ensure comments are fully loaded
submission_LittleMermaid_2.comments.replace_more(limit=None)

# Flatten all comments in the comment forest
all_comments_LittleMermaid_2 = submission_LittleMermaid_2.comments.list()

# Initialize an empty list to store comment data
comment_data_LittleMermaid_2 = []

# Extract and print details for 100 comments
for i, comment in enumerate(all_comments_LittleMermaid_2[:100]):  # Get first 100 comments
    author = comment.author.name if comment.author else "[Deleted]"  # Handle deleted authors
    comment_body = comment.body
    word_count = len(comment_body.split())
    comment_data_LittleMermaid_2.append({
        "Author": author,
        "Comment": comment_body,
        "Word Count": word_count,
        "Score": comment.score
    })
# Step 6: Convert the list into a Pandas DataFrame
import pandas as pd  # Import pandas for DataFrame
LittleMermaid_2= pd.DataFrame(comment_data_LittleMermaid_2)

secret="K-023J8b3hUeew_vvP1tg9fZPWoMfg"
APP_ID="Cjjl_-MnS2K8_YcfXQVpZg"
reddit= praw.Reddit(
    check_for_async=False,
    client_id=APP_ID,
    client_secret=secret,
    user_agent="Comment Extraction")
submission_url_LittleMermaid_3 = "https://www.reddit.com/r/disney/comments/11pvnck/official_poster_for_disneys_the_little_mermaid/"
submission_LittleMermaid_3 = reddit.submission(url=submission_url_LittleMermaid_3)
# Ensure comments are fully loaded
submission_LittleMermaid_3.comments.replace_more(limit=None)

# Flatten all comments in the comment forest
all_comments_LittleMermaid_3 = submission_LittleMermaid_3.comments.list()

# Initialize an empty list to store comment data
comment_data_LittleMermaid_3 = []

# Extract and print details for 100 comments
for i, comment in enumerate(all_comments_LittleMermaid_3[:100]):  # Get first 100 comments
    author = comment.author.name if comment.author else "[Deleted]"  # Handle deleted authors
    comment_body = comment.body
    word_count = len(comment_body.split())
    comment_data_LittleMermaid_3.append({
        "Author": author,
        "Comment": comment_body,
        "Word Count": word_count,
        "Score": comment.score
    })
# Step 6: Convert the list into a Pandas DataFrame
import pandas as pd  # Import pandas for DataFrame
LittleMermaid_3= pd.DataFrame(comment_data_LittleMermaid_3)

#Merge all dataframes into one single Dataframe
merged_Reddit_df= pd.concat([Snow_White_1, Snow_White_2, Snow_White_3, LiloAndStitch_1, LiloAndStitch_2, LiloAndStitch_3, LittleMermaid, LittleMermaid_2, LittleMermaid_3], ignore_index=True)

#Save final dataframe as csv
merged_Reddit_df.to_csv("merged_Reddit_df.csv", index=False)

#Pre-processing 1, expansion of contractions
#import classes
!pip install contractions
import contractions
#Create function
def text_with_contractions(text):
    words = []
    for word in text.split():
        expanded = contractions.fix(word)
        words.append(expanded)
    return ' '.join(words)
# Apply the function to each comment
merged_Reddit_df['Comment'] = merged_Reddit_df['Comment'].apply(text_with_contractions)

# Preview results
print(merged_Reddit_df[['Author', 'Comment']].head())

#Pre-Processing 2, Standardise text cases
#Create Function
def text_lowercase(text):
  return text.lower()
# Apply the function to each comment
merged_Reddit_df['Comment'] = merged_Reddit_df['Comment'].apply(text_lowercase)

#Pre-Processing 3, Text substitution
#Import class
import re
#Create function
def text_substitution(text):
    # 1. Remove URLs
    text = re.sub(r'http\S+|www\S+', 'URL', text)
    # 2. Remove Reddit user and subreddit mentions
    text = re.sub(r'u\/\S+', '${USER}', text)
    text = re.sub(r'r\/\S+', '${Subreddit}', text)
    # 3. Substitute movie ratings (out of 10 and out of 5)
    text = re.sub(r'\b(10\/10|9\/10|10 out of 10|9 out of 10|5\/5|4\.5\/5)\b', 'very_positive_rating', text)
    text = re.sub(r'\b(7\/10|6\/10|7 out of 10|6 out of 10|4\/5|3\.5\/5)\b', 'neutral_rating', text)
    text = re.sub(r'\b(5\/10|4\/10|3\/10|5 out of 10|4 out of 10|3 out of 10|3\/5|2\.5\/5)\b', 'negative_rating', text)
    text = re.sub(r'\b(2\/10|1\/10|0\/10|2 out of 10|1 out of 10|0 out of 10|2\/5|1\.5\/5|1\/5|0\.5\/5|0\/5)\b', 'very_negative_rating', text)
    #remove unneccessary special characters
    text = re.sub(r'[^a-zA-Z0-9\s\.\,\!\?]', '', text)
    #extra space
    text = re.sub(r'\s+', ' ', text).strip()
    return text
#Apply Function
merged_Reddit_df['Comment'] = merged_Reddit_df['Comment'].apply(text_substitution)

#Pre-Processing 4, Tokenisation
#Import classes
!pip install --upgrade nltk
import nltk
nltk.download('punkt_tab')
nltk.download('punkt')
from nltk.tokenize import word_tokenize
#Create Function
def text_to_tokens(text):
    tokens = []
    for word in word_tokenize(text):
        tokens.append(word)
    return tokens

# Fix your Comment column to string
merged_Reddit_df['Comment'] = merged_Reddit_df['Comment'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))
#Apply function
merged_Reddit_df['Comment'] = merged_Reddit_df['Comment'].apply(text_to_tokens)

# Preview
print(merged_Reddit_df[['Author', 'Comment']].head())

#Pre-Processing 5, Removal of stopwords
#Import classes
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
#Create function
def tokens_without_stopwords(tokens):
    stop_words = set(stopwords.words('english'))
    keep = []
    for token in tokens:
        if token.lower() not in stop_words:
            keep.append(token)
    return keep
#Apply Function
merged_Reddit_df['Comment'] = merged_Reddit_df['Comment'].apply(tokens_without_stopwords)

# Preprocessing 6, Lemmatisation
#Import classes
import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.stem import WordNetLemmatizer
#Create Function
def apply_lemmatisation(tokens):
    keep = []
    lemmatizer = WordNetLemmatizer()
    for token in tokens:
        keep.append(lemmatizer.lemmatize(token))
    return keep
#Apply Function
merged_Reddit_df['Comment'] = merged_Reddit_df['Comment'].apply(apply_lemmatisation)

# Data Exploration 1, Term Frequency and Word Cloud
#Import classes
!pip install wordcloud
from wordcloud import WordCloud
import matplotlib.pyplot as plt
all_tokens = []
#Flatten all comments into a single string
for comment in merged_Reddit_df['Comment']:
    all_tokens.extend(comment)
text = ' '.join(all_tokens)
#Create and display a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Most Frequent Terms in r/Disney Comments')
plt.show()

#Data Exploration 2, Noun-phrase Extraction
#Import classes
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('brown')
!pip install textblob
from textblob import TextBlob
from collections import Counter

#Create function
def extract_noun_phrases_textblob(comment):

  if isinstance(comment, list):
        comment = ' '.join(comment)
        blob = TextBlob(comment)
        return blob.noun_phrases

# Apply to your dataset
merged_Reddit_df['Noun Phrases'] = merged_Reddit_df['Comment'].apply(extract_noun_phrases_textblob)

# Flatten the list of noun phrases from all rows
all_noun_phrases = [phrase.lower() for sublist in merged_Reddit_df['Noun Phrases'] for phrase in sublist]

# Count the frequency of each noun phrase
noun_phrase_counts = Counter(all_noun_phrases)

# Get the top 10 most common noun phrases
top_10_noun_phrases = noun_phrase_counts.most_common(10)

# Display the results
for phrase, count in top_10_noun_phrases:
    print(f"{phrase}: {count}")

#Create horizontal bar chart plot for Noun-phrase frequency
import matplotlib.pyplot as plt

# Get the noun phrases and their counts separately
phrases, counts = zip(*top_10_noun_phrases)

# Create the horizontal bar chart
plt.figure(figsize=(10, 6))
plt.barh(phrases, counts, color='skyblue')
plt.xlabel("Frequency")
plt.title("Top 10 Most Frequent Noun Phrases in r/Disney Comments")
plt.gca().invert_yaxis()  # Highest frequency on top
plt.tight_layout()
plt.show()

#Data exploration 3, Topic Modelling
# Step 1: Convert token list to string
merged_Reddit_df['Comment'] = merged_Reddit_df['Comment'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))
#Import classes
from sklearn.feature_extraction.text import CountVectorizer
#Convert text data into a document-term matrix
vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
doc_term_matrix = vectorizer.fit_transform(merged_Reddit_df['Comment'])
from sklearn.decomposition import LatentDirichletAllocation
#Initialise LDA model to train
lda_model = LatentDirichletAllocation(n_components=5, random_state=42)
#Fit the model
lda_model.fit(doc_term_matrix)
#Display the topics
def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print(f"Topic {topic_idx+1}:")
        print(", ".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))
        print()

feature_names = vectorizer.get_feature_names_out()
display_topics(lda_model, feature_names, 10)

#Explore Topic distributions
#Get topic distribution per document
lda_output = lda_model.transform(doc_term_matrix)  # Shape: (num_docs, num_topics)

#Turn it into a DataFrame for readability
topic_distribution_df = pd.DataFrame(lda_output, columns=[f"Topic {i+1}" for i in range(lda_model.n_components)])

#Add the original comment next to it
topic_distribution_df['Comment'] = merged_Reddit_df['Comment'].values

#Data Exploration 4, Sentiment Analysis
#Import classes
!pip install vaderSentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyser = SentimentIntensityAnalyzer()

#Function to extract sentiment score
def vader_sentiment_scores(comment):
    if isinstance(comment, list):
        comment = ' '.join(comment)

    scores = analyser.polarity_scores(comment)
    return scores
vader_scores = merged_Reddit_df['Comment'].apply(vader_sentiment_scores).apply(pd.Series)

#Join vadar score to original dataframe
merged_Reddit_df = pd.concat([merged_Reddit_df, vader_scores], axis=1)

#Create function to create compounded sentiment score labels
def label_sentiment(compound):
    if compound >= 0.05:
        return 'positive'
    elif compound <= -0.05:
        return 'negative'
    else:
        return 'neutral'
#Apply function
merged_Reddit_df['Sentiment'] = merged_Reddit_df['compound'].apply(label_sentiment)
#Preview
pd.set_option('display.max_colwidth', None)
print(merged_Reddit_df[['Author', 'Comment', 'compound', 'Sentiment']].head(10))

#Create weighted sentiment scores using upvote score
merged_Reddit_df['Weighted_Sentiment'] = merged_Reddit_df['compound'] * merged_Reddit_df['Score']
#Create score thresholds
high_score_comments = merged_Reddit_df[merged_Reddit_df['Score'] >= 10]
low_score_comments = merged_Reddit_df[merged_Reddit_df['Score'] <= 0]
#Find what sentiments are getting the highest overall upvote score
high_score_sentiment= high_score_comments['compound'].mean()
low_score_sentiment= low_score_comments['compound'].mean()
#Mean of high score vs low score results.
high_score_sentiment, low_score_sentiment

#Create plot that shows Compound sentiment vs upvote score as well as showing weighted sentiment.
plt.figure(figsize=(8, 5))
plt.scatter(merged_Reddit_df['compound'], merged_Reddit_df['Score'],
            c=merged_Reddit_df['Weighted_Sentiment'], cmap='coolwarm', alpha=0.6)
plt.colorbar(label='Weighted Sentiment')
plt.title('Sentiment vs Upvote Score')
plt.xlabel('Compound Sentiment')
plt.ylabel('Upvote Score')
plt.show()

merged_Reddit_df.to_csv("reddit_sentiment_results.csv", index=False)

#Create plot to show distribution of sentiment
# Count the number of comments per sentiment category
sentiment_counts = merged_Reddit_df['Sentiment'].value_counts()
# Plot
plt.figure(figsize=(6,4))
sentiment_counts.plot(kind='bar', color=['green', 'red', 'grey'])
plt.title("Distribution of Sentiment Labels")
plt.xlabel("Sentiment")
plt.ylabel("Number of Comments")
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

#Create plot to show distribution of VADER compound scores
plt.figure(figsize=(8,4))
plt.hist(merged_Reddit_df['compound'], bins=30, color='skyblue', edgecolor='black')
plt.title("Distribution of VADER Compound Scores")
plt.xlabel("Compound Score")
plt.ylabel("Number of Comments")
plt.tight_layout()
plt.show()